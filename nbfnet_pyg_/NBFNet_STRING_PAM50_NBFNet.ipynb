{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a2aeb0",
   "metadata": {},
   "source": [
    "\n",
    "# NBFNet on STRING PPI (True NBFNet) + PAM50 Probe\n",
    "\n",
    "This notebook:\n",
    "1) Downloads **STRING PPI** (human; configurable threshold)\n",
    "2) Builds triples and train/valid/test splits\n",
    "3) **Clones & imports the real NBFNet** from `KiddoZhu/NBFNet-PyG` (no dot-product fallback)\n",
    "4) Trains & evaluates a 6-layer NBFNet on the PPI KG\n",
    "5) **Probes PAM50 genes**: for each of the 50, ranks likely partners\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31fce2e",
   "metadata": {},
   "source": [
    "## 1) Imports & global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e878129b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 항상 맨 첫 셀에서 실행!\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # ★ 변경점: 반드시 0,1만 노출\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac9f75bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5752f928f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== Global settings ====\n",
    "DATA_ROOT = Path(\"./data_string\"); DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "RUN_DIR = Path(\"./nbfnet_runs\"); RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EXPT_NAME = \"string_ppi_interact\"\n",
    "\n",
    "# STRING\n",
    "SPECIES_ID = 9606  # Human\n",
    "CONFIDENCE_THRESHOLD = 800  # 0..1000\n",
    "MAX_EDGES = None           # set to int to cap edges for quick dev\n",
    "RANDOM_SEED = 42\n",
    "TRAIN_RATIO, VALID_RATIO, TEST_RATIO = 0.85 , 0.05, 0.10\n",
    "\n",
    "# NBFNet hyperparams (6 layers)\n",
    "INPUT_DIM = 64\n",
    "HIDDEN_DIMS = [64,64,64,64,64,64]   # 6 layers\n",
    "# (참고) 아래에서 정/역 관계 2개를 쓸 것이므로 여기의 NUM_RELATIONS는 사용하지 않음\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 4096\n",
    "LR = 2e-3\n",
    "NUM_NEG = 16\n",
    "TOPK = 20\n",
    "MESSAGE_FUNCT = \"distmult\"\n",
    "AGGREGATE_FUNC = \"pna\"\n",
    "SHORTCUT = True\n",
    "LAYER_NORM = True\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c542f69f",
   "metadata": {},
   "source": [
    "## 2) Download STRING (aliases + links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6856d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready: data_string/9606.protein.aliases.txt.gz data_string/9606.protein.links.txt.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "def download_with_fallback(urls, dest_path):\n",
    "    dest_path = Path(dest_path)\n",
    "    dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    for u in urls:\n",
    "        try:\n",
    "            print(f\"Trying: {u}\")\n",
    "            with requests.get(u, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                tmp = dest_path.with_suffix(dest_path.suffix + \".part\")\n",
    "                with open(tmp, \"wb\") as f:\n",
    "                    shutil.copyfileobj(r.raw, f)\n",
    "                tmp.rename(dest_path)\n",
    "            print(f\"Downloaded -> {dest_path}\")\n",
    "            return dest_path\n",
    "        except Exception as e:\n",
    "            print(\"  Failed:\", e)\n",
    "    raise RuntimeError(\"All URL candidates failed.\")\n",
    "\n",
    "def string_aliases_urls(species):\n",
    "    return [\n",
    "        f\"https://stringdb-static.org/download/protein.aliases.v12.0/{species}.protein.aliases.v12.0.txt.gz\",\n",
    "        f\"https://stringdb-static.org/download/protein.aliases.v11.5/{species}.protein.aliases.v11.5.txt.gz\",\n",
    "    ]\n",
    "\n",
    "def string_links_urls(species):\n",
    "    return [\n",
    "        f\"https://stringdb-static.org/download/protein.links.full.v12.0/{species}.protein.links.full.v12.0.txt.gz\",\n",
    "        f\"https://stringdb-static.org/download/protein.links.detailed.v11.5/{species}.protein.links.detailed.v11.5.txt.gz\",\n",
    "        f\"https://stringdb-static.org/download/protein.links.v11.5/{species}.protein.links.v11.5.txt.gz\",\n",
    "    ]\n",
    "\n",
    "ALIASES_GZ = DATA_ROOT / f\"{SPECIES_ID}.protein.aliases.txt.gz\"\n",
    "LINKS_GZ   = DATA_ROOT / f\"{SPECIES_ID}.protein.links.txt.gz\"\n",
    "\n",
    "if not ALIASES_GZ.exists():\n",
    "    download_with_fallback(string_aliases_urls(SPECIES_ID), ALIASES_GZ)\n",
    "\n",
    "if not LINKS_GZ.exists():\n",
    "    download_with_fallback(string_links_urls(SPECIES_ID), LINKS_GZ)\n",
    "\n",
    "print(\"Ready:\", ALIASES_GZ, LINKS_GZ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a1b127",
   "metadata": {},
   "source": [
    "## 3) Parse aliases → symbol ↔ protein id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "903fa2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19699, 19691)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, gzip\n",
    "import pandas as pd\n",
    "\n",
    "def read_aliases_gz(path, prefer_sources=(\"Ensembl_HGNC\", \"HGNC\", \"Ensembl\", \"UniProtKB\")):\n",
    "    rows = []\n",
    "    with gzip.open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#\"): continue\n",
    "            parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            if len(parts) < 3: \n",
    "                continue\n",
    "            prot, alias, source = parts[0], parts[1], parts[2]\n",
    "            rows.append((prot, alias, source))\n",
    "    df = pd.DataFrame(rows, columns=[\"protein_id\", \"alias\", \"source\"])\n",
    "    df[\"is_symbol_like\"] = df[\"alias\"].str.match(r\"^[A-Z0-9._-]+$\")\n",
    "    pref = df[\"source\"].apply(lambda s: (prefer_sources.index(s) if s in prefer_sources else len(prefer_sources)))\n",
    "    df[\"pref_rank\"] = pref\n",
    "    sym_df = df[df[\"is_symbol_like\"]].copy()\n",
    "    best_alias = sym_df.sort_values([\"protein_id\",\"pref_rank\"]).groupby(\"protein_id\").first().reset_index()\n",
    "    prot_to_sym = dict(zip(best_alias[\"protein_id\"], best_alias[\"alias\"]))\n",
    "    sym_to_prot = {}\n",
    "    for _, r in best_alias.iterrows():\n",
    "        sym = r[\"alias\"]; pid = r[\"protein_id\"]\n",
    "        if sym not in sym_to_prot:\n",
    "            sym_to_prot[sym] = pid\n",
    "    return prot_to_sym, sym_to_prot\n",
    "\n",
    "prot_to_sym, sym_to_prot = read_aliases_gz(ALIASES_GZ)\n",
    "len(prot_to_sym), len(sym_to_prot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c88553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[counts] prot_to_sym=19,699, sym_to_prot=19,691\n",
      "\n",
      "[preview] prot_to_sym:\n",
      "  9606.ENSP00000000233  ->  ARF5\n",
      "  9606.ENSP00000000412  ->  M6PR\n",
      "  9606.ENSP00000001008  ->  FKBP4\n",
      "  9606.ENSP00000001146  ->  CYP26B1\n",
      "  9606.ENSP00000002125  ->  NDUFAF7\n",
      "  9606.ENSP00000002165  ->  FUCA2\n",
      "  9606.ENSP00000002596  ->  HS3ST1\n",
      "  9606.ENSP00000002829  ->  SEMA3F\n",
      "  9606.ENSP00000003084  ->  CFTR\n",
      "  9606.ENSP00000003100  ->  CYP51A1\n",
      "\n",
      "[preview] sym_to_prot:\n",
      "  A1BG  ->  9606.ENSP00000263100\n",
      "  A1CF  ->  9606.ENSP00000378868\n",
      "  A2M  ->  9606.ENSP00000323929\n",
      "  A2ML1  ->  9606.ENSP00000299698\n",
      "  A3GALT2  ->  9606.ENSP00000475261\n",
      "  A4GALT  ->  9606.ENSP00000384794\n",
      "  A4GNT  ->  9606.ENSP00000236709\n",
      "  AAAS  ->  9606.ENSP00000209873\n",
      "  AACS  ->  9606.ENSP00000324842\n",
      "  AADAC  ->  9606.ENSP00000232892\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_id</th>\n",
       "      <th>symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9606.ENSP00000000233</td>\n",
       "      <td>ARF5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9606.ENSP00000000412</td>\n",
       "      <td>M6PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9606.ENSP00000001008</td>\n",
       "      <td>FKBP4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9606.ENSP00000001146</td>\n",
       "      <td>CYP26B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9606.ENSP00000002125</td>\n",
       "      <td>NDUFAF7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9606.ENSP00000002165</td>\n",
       "      <td>FUCA2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9606.ENSP00000002596</td>\n",
       "      <td>HS3ST1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9606.ENSP00000002829</td>\n",
       "      <td>SEMA3F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9606.ENSP00000003084</td>\n",
       "      <td>CFTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9606.ENSP00000003100</td>\n",
       "      <td>CYP51A1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             protein_id   symbol\n",
       "0  9606.ENSP00000000233     ARF5\n",
       "1  9606.ENSP00000000412     M6PR\n",
       "2  9606.ENSP00000001008    FKBP4\n",
       "3  9606.ENSP00000001146  CYP26B1\n",
       "4  9606.ENSP00000002125  NDUFAF7\n",
       "5  9606.ENSP00000002165    FUCA2\n",
       "6  9606.ENSP00000002596   HS3ST1\n",
       "7  9606.ENSP00000002829   SEMA3F\n",
       "8  9606.ENSP00000003084     CFTR\n",
       "9  9606.ENSP00000003100  CYP51A1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>protein_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1BG</td>\n",
       "      <td>9606.ENSP00000263100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1CF</td>\n",
       "      <td>9606.ENSP00000378868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2M</td>\n",
       "      <td>9606.ENSP00000323929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2ML1</td>\n",
       "      <td>9606.ENSP00000299698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3GALT2</td>\n",
       "      <td>9606.ENSP00000475261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A4GALT</td>\n",
       "      <td>9606.ENSP00000384794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A4GNT</td>\n",
       "      <td>9606.ENSP00000236709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AAAS</td>\n",
       "      <td>9606.ENSP00000209873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AACS</td>\n",
       "      <td>9606.ENSP00000324842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AADAC</td>\n",
       "      <td>9606.ENSP00000232892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    symbol            protein_id\n",
       "0     A1BG  9606.ENSP00000263100\n",
       "1     A1CF  9606.ENSP00000378868\n",
       "2      A2M  9606.ENSP00000323929\n",
       "3    A2ML1  9606.ENSP00000299698\n",
       "4  A3GALT2  9606.ENSP00000475261\n",
       "5   A4GALT  9606.ENSP00000384794\n",
       "6    A4GNT  9606.ENSP00000236709\n",
       "7     AAAS  9606.ENSP00000209873\n",
       "8     AACS  9606.ENSP00000324842\n",
       "9    AADAC  9606.ENSP00000232892"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gzip, random\n",
    "import pandas as pd\n",
    "\n",
    "def preview_mappings(prot_to_sym, sym_to_prot, n=10, random_sample=False):\n",
    "    print(f\"[counts] prot_to_sym={len(prot_to_sym):,}, sym_to_prot={len(sym_to_prot):,}\")\n",
    "\n",
    "    if not prot_to_sym or not sym_to_prot:\n",
    "        print(\"\\n[debug] mappings are empty. Showing first few raw lines from the gz file…\")\n",
    "        with gzip.open(ALIASES_GZ, \"rt\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            for i, line in zip(range(10), f):\n",
    "                print(line.rstrip(\"\\n\"))\n",
    "        return\n",
    "\n",
    "    if random_sample:\n",
    "        pts_items = random.sample(list(prot_to_sym.items()), min(n, len(prot_to_sym)))\n",
    "        stp_items = random.sample(list(sym_to_prot.items()), min(n, len(sym_to_prot)))\n",
    "    else:\n",
    "        pts_items = sorted(prot_to_sym.items(), key=lambda kv: kv[0])[:n]\n",
    "        stp_items = sorted(sym_to_prot.items(), key=lambda kv: kv[0])[:n]\n",
    "\n",
    "    print(\"\\n[preview] prot_to_sym:\")\n",
    "    for k, v in pts_items:\n",
    "        print(f\"  {k}  ->  {v}\")\n",
    "\n",
    "    print(\"\\n[preview] sym_to_prot:\")\n",
    "    for k, v in stp_items:\n",
    "        print(f\"  {k}  ->  {v}\")\n",
    "\n",
    "    display(pd.DataFrame(pts_items, columns=[\"protein_id\", \"symbol\"]).head(n))\n",
    "    display(pd.DataFrame(stp_items, columns=[\"symbol\", \"protein_id\"]).head(n))\n",
    "\n",
    "prot_to_sym, sym_to_prot = read_aliases_gz(ALIASES_GZ)\n",
    "preview_mappings(prot_to_sym, sym_to_prot, n=10, random_sample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c2de547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#nodes=14481, #edges=162305\n"
     ]
    }
   ],
   "source": [
    "def read_links_gz(path, score_threshold=CONFIDENCE_THRESHOLD, max_edges=MAX_EDGES):\n",
    "    rows = []\n",
    "    with gzip.open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        header = f.readline().strip().split()\n",
    "        if len(header) < 3:\n",
    "            f.seek(0); header = None\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            if line.startswith(\"#\"): continue\n",
    "            parts = line.strip().split()\n",
    "            if header is None:\n",
    "                if len(parts) < 3: continue\n",
    "                p1, p2, sc = parts[0], parts[1], parts[2]\n",
    "            else:\n",
    "                def col(name, default_idx=-1):\n",
    "                    try:\n",
    "                        return parts[header.index(name)]\n",
    "                    except Exception:\n",
    "                        return parts[default_idx] if default_idx >= 0 else None\n",
    "                p1 = col(\"protein1\", 0)\n",
    "                p2 = col(\"protein2\", 1)\n",
    "                sc = None\n",
    "                for c in [\"combined_score\",\"experimental\",\"experimental_score\",\"score\"]:\n",
    "                    if c in header:\n",
    "                        try:\n",
    "                            sc = parts[header.index(c)]; break\n",
    "                        except: pass\n",
    "                if sc is None: sc = parts[-1]\n",
    "            try: sc = int(float(sc))\n",
    "            except: continue\n",
    "            if sc >= score_threshold:\n",
    "                rows.append((p1, p2, sc))\n",
    "                count += 1\n",
    "                if max_edges is not None and count >= max_edges:\n",
    "                    break\n",
    "    df = pd.DataFrame(rows, columns=[\"protein1\",\"protein2\",\"score\"])\n",
    "    df = df[df[\"protein1\"] != df[\"protein2\"]].copy()\n",
    "    df[\"pair\"] = df.apply(lambda r: tuple(sorted((r[\"protein1\"], r[\"protein2\"]))), axis=1)\n",
    "    df = df.drop_duplicates(\"pair\").drop(columns=[\"pair\"])\n",
    "    return df\n",
    "\n",
    "links_df = read_links_gz(LINKS_GZ)\n",
    "proteins = pd.Index(pd.unique(links_df[[\"protein1\",\"protein2\"]].values.ravel()))\n",
    "pid_to_idx = {pid:i for i,pid in enumerate(proteins)}\n",
    "idx_to_pid = {i:pid for pid,i in pid_to_idx.items()}\n",
    "\n",
    "edges = np.array([(pid_to_idx[p1], pid_to_idx[p2]) for p1,p2 in links_df[[\"protein1\",\"protein2\"]].values], dtype=np.int64)\n",
    "print(f\"#nodes={len(proteins)}, #edges={len(edges)}\")\n",
    "\n",
    "(pd.Series(proteins, name=\"protein_id\").to_frame()\n",
    " .assign(gene_symbol=lambda df: df[\"protein_id\"].map(lambda x: prot_to_sym.get(x, None)))\n",
    " .to_csv(DATA_ROOT/\"protein_index_map.csv\", index_label=\"node_idx\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5447114f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275918, 16230, 32462)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REL = 0  # single relation id\n",
    "\n",
    "def make_triples(edges, both=True):\n",
    "    triples = []\n",
    "    for u,v in edges:\n",
    "        triples.append((u, REL, v))\n",
    "        if both:\n",
    "            triples.append((v, REL, u))\n",
    "    return np.array(triples, dtype=np.int64)\n",
    "\n",
    "triples = make_triples(edges, both=True)\n",
    "num = len(triples)\n",
    "perm = np.random.permutation(num)\n",
    "n_train = int(num * TRAIN_RATIO)\n",
    "n_valid = int(num * VALID_RATIO)\n",
    "train_triples = triples[perm[:n_train]]\n",
    "valid_triples = triples[perm[n_train:n_train+n_valid]]\n",
    "test_triples  = triples[perm[n_train+n_valid:]]\n",
    "len(train_triples), len(valid_triples), len(test_triples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "988045fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 324610], edge_type=[324610], num_nodes=14481, num_edges=324610, num_relations=2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# direct relation id = 0, inverse relation id = 1\n",
    "NUM_RELATIONS = 2\n",
    "\n",
    "edge_index_fwd = torch.as_tensor(edges.T, dtype=torch.long)   # [2, E]\n",
    "edge_index_rev = edge_index_fwd.flip(0)                       # [2, E] v->u\n",
    "\n",
    "edge_index = torch.cat([edge_index_fwd, edge_index_rev], dim=1)  # [2, 2E]\n",
    "edge_type  = torch.cat([\n",
    "    torch.zeros(edge_index_fwd.size(1), dtype=torch.long),       # 0\n",
    "    torch.ones(edge_index_rev.size(1), dtype=torch.long)         # 1\n",
    "], dim=0)\n",
    "\n",
    "data = Data(edge_index=edge_index, edge_type=edge_type)\n",
    "data.num_nodes = len(proteins)\n",
    "data.num_edges = edge_index.size(1)\n",
    "data.num_relations = NUM_RELATIONS\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b68c55fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PyG MessagePassing init shim (RUN ONCE, before creating the model) ===\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "# 기존 __init__을 감싸서, 인스턴스 생성 시 구버전 필드명들도 채워준다.\n",
    "if not hasattr(MessagePassing, \"_nbfnet_init_shim_applied\"):\n",
    "    _orig_init = MessagePassing.__init__\n",
    "\n",
    "    def _init_shim(self, *args, **kwargs):\n",
    "        _orig_init(self, *args, **kwargs)\n",
    "        # 최신 필드명을 구버전 이름으로 복사\n",
    "        if hasattr(self, \"_fused_user_args\") and not hasattr(self, \"__fused_user_args__\"):\n",
    "            self.__fused_user_args__ = self._fused_user_args\n",
    "        if hasattr(self, \"_user_args\") and not hasattr(self, \"__user_args__\"):\n",
    "            self.__user_args__ = self._user_args\n",
    "        if hasattr(self, \"_special_args\") and not hasattr(self, \"__special_args__\"):\n",
    "            self.__special_args__ = self._special_args\n",
    "\n",
    "    MessagePassing.__init__ = _init_shim\n",
    "    MessagePassing._nbfnet_init_shim_applied = True\n",
    "\n",
    "# map new (single-underscore) names -> old (double-underscore) names expected by nbfnet-pyg\n",
    "if not hasattr(MessagePassing, \"__check_input__\") and hasattr(MessagePassing, \"_check_input\"):\n",
    "    MessagePassing.__check_input__ = MessagePassing._check_input\n",
    "if not hasattr(MessagePassing, \"__collect__\") and hasattr(MessagePassing, \"_collect\"):\n",
    "    MessagePassing.__collect__ = MessagePassing._collect\n",
    "if not hasattr(MessagePassing, \"__fused_user_args__\") and hasattr(MessagePassing, \"_fused_user_args\"):\n",
    "    MessagePassing.__fused_user_args__ = MessagePassing._fused_user_args\n",
    "# 추가: size 처리\n",
    "if not hasattr(MessagePassing, \"__set_size__\") and hasattr(MessagePassing, \"_set_size\"):\n",
    "    MessagePassing.__set_size__ = MessagePassing._set_size\n",
    "# 일부 PyG 버전에서는 lift도 단일 밑줄로 제공\n",
    "if not hasattr(MessagePassing, \"__lift__\") and hasattr(MessagePassing, \"_lift\"):\n",
    "    MessagePassing.__lift__ = MessagePassing._lift\n",
    "# === Torch sparse_csr/csc compatibility shim (RUN ONCE before training) ===\n",
    "import torch\n",
    "\n",
    "# PyTorch<1.10 에는 sparse_csr / sparse_csc 속성이 없음 → 더미 속성으로 막아줌\n",
    "if not hasattr(torch, \"sparse_csr\"):\n",
    "    torch.sparse_csr = object()\n",
    "if not hasattr(torch, \"sparse_csc\"):\n",
    "    torch.sparse_csc = object()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ff5d7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use gcc : /data2/project/bin_jip/miniconda3/envs/nbf37/bin/x86_64-conda-linux-gnu-gcc\n",
      "use g++ : /data2/project/bin_jip/miniconda3/envs/nbf37/bin/x86_64-conda-linux-gnu-g++\n",
      "libstdc++ inc base: /data2/project/bin_jip/miniconda3/envs/nbf37/x86_64-conda-linux-gnu/include/c++/10.4.0 True\n",
      "libstdc++ inc tgt : /data2/project/bin_jip/miniconda3/envs/nbf37/x86_64-conda-linux-gnu/include/c++/10.4.0/x86_64-conda-linux-gnu True\n",
      "nvcc: /data2/project/bin_jip/miniconda3/envs/nbf37/bin/nvcc\n",
      "ninja: /data2/project/bin_jip/miniconda3/envs/nbf37/bin/ninja\n",
      "TORCH_CUDA_ARCH_LIST: 8.6\n",
      "TORCH_EXTENSIONS_DIR: /data2/project/bin_jip/nbfnet_pyg/nbfnet_runs/torch_ext\n",
      "OK: cpp_extension.load single-patched (toolchain pinned)\n"
     ]
    }
   ],
   "source": [
    "# ==== Fix nvcc toolchain for rspmm build (RUN ONCE, before importing NBFNet) ====\n",
    "import os, sys, shutil, subprocess, importlib\n",
    "from pathlib import Path\n",
    "import torch.utils.cpp_extension as cpp_ext\n",
    "\n",
    "# 0) 기존 중복 패치 제거\n",
    "cpp_ext = importlib.reload(cpp_ext)\n",
    "\n",
    "# 1) conda의 gcc-10 / g++-10 우선 선택 (없으면 일반 conda gcc, 마지막엔 시스템 g++까지)\n",
    "PREFIX   = Path(sys.prefix)\n",
    "cand_gcc = [\n",
    "    PREFIX/\"bin/x86_64-conda-linux-gnu-gcc-10\",\n",
    "    PREFIX/\"bin/x86_64-conda-linux-gnu-gcc\",\n",
    "    Path(\"/usr/bin/gcc-10\"),\n",
    "    Path(\"/usr/bin/gcc\"),\n",
    "]\n",
    "cand_gxx = [\n",
    "    PREFIX/\"bin/x86_64-conda-linux-gnu-g++-10\",\n",
    "    PREFIX/\"bin/x86_64-conda-linux-gnu-g++\",\n",
    "    Path(\"/usr/bin/g++-10\"),\n",
    "    Path(\"/usr/bin/g++\"),\n",
    "]\n",
    "CONDAGCC = next((p for p in cand_gcc if p.exists()), cand_gcc[-1])\n",
    "CONDAGXX = next((p for p in cand_gxx if p.exists()), cand_gxx[-1])\n",
    "print(\"use gcc :\", CONDAGCC)\n",
    "print(\"use g++ :\", CONDAGXX)\n",
    "assert CONDAGCC.exists() and CONDAGXX.exists(), \"conda/system gcc/g++ not found\"\n",
    "\n",
    "# 2) libstdc++ include 경로 (선택된 컴파일러 버전에 맞춰 계산)\n",
    "def _detect_includes(gcc_path: Path):\n",
    "    if gcc_path.name.endswith(\"gcc-10\"):\n",
    "        base = Path(\"/usr/include/c++/10\")\n",
    "        tgt  = Path(\"/usr/include/x86_64-linux-gnu/c++/10\")\n",
    "        return base, tgt\n",
    "    ver = subprocess.check_output([str(gcc_path), \"-dumpversion\"]).decode().strip()\n",
    "    base = PREFIX / \"x86_64-conda-linux-gnu\" / \"include\" / \"c++\" / ver\n",
    "    tgt  = base / \"x86_64-conda-linux-gnu\"\n",
    "    return base, tgt\n",
    "\n",
    "INC_BASE, INC_TGT = _detect_includes(CONDAGCC)\n",
    "print(\"libstdc++ inc base:\", INC_BASE, INC_BASE.exists())\n",
    "print(\"libstdc++ inc tgt :\", INC_TGT , INC_TGT.exists())\n",
    "\n",
    "# (추가) conda nvcc가 우선 잡히도록 PATH 앞에 붙임\n",
    "os.environ[\"PATH\"] = f\"{str(Path(sys.prefix)/'bin')}:{os.environ['PATH']}\"\n",
    "\n",
    "# (추가) conda CUDA 헤더 경로를 -isystem으로 같이 주기 위해 기억\n",
    "CUDA_INC_CONDA = Path(sys.prefix) / \"targets\" / \"x86_64-linux\" / \"include\"\n",
    "\n",
    "# 3) nvcc / ninja 확인\n",
    "nvcc  = shutil.which(\"nvcc\")\n",
    "ninja = shutil.which(\"ninja\") or shutil.which(\"ninja-build\")\n",
    "print(\"nvcc:\", nvcc)\n",
    "print(\"ninja:\", ninja)\n",
    "assert nvcc  is not None, \"nvcc not found (e.g., `conda install -y cuda-nvcc`)\"\n",
    "assert ninja is not None, \"ninja not found (e.g., `conda install -y ninja` or `pip install ninja`)\"\n",
    "\n",
    "# 4) 환경변수 고정\n",
    "os.environ[\"CC\"]          = str(CONDAGCC)\n",
    "os.environ[\"CXX\"]         = str(CONDAGXX)\n",
    "os.environ[\"CUDAHOSTCXX\"] = str(CONDAGXX)\n",
    "os.environ.setdefault(\"TORCH_CUDA_ARCH_LIST\", \"8.6\")  # RTX A6000\n",
    "TORCH_EXT_DIR = (Path(\"./nbfnet_runs/torch_ext\").resolve())\n",
    "os.environ[\"TORCH_EXTENSIONS_DIR\"] = str(TORCH_EXT_DIR)\n",
    "print(\"TORCH_CUDA_ARCH_LIST:\", os.environ[\"TORCH_CUDA_ARCH_LIST\"])\n",
    "print(\"TORCH_EXTENSIONS_DIR:\", os.environ[\"TORCH_EXTENSIONS_DIR\"])\n",
    "\n",
    "# 5) 실패 캐시 정리\n",
    "shutil.rmtree(os.path.expanduser(\"~/.cache/torch_extensions\"), ignore_errors=True)\n",
    "shutil.rmtree(TORCH_EXT_DIR, ignore_errors=True)\n",
    "TORCH_EXT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 6) cpp_extension.load 단일 패치:\n",
    "#    - 기존 -ccbin(공백형/=/형 모두) 제거\n",
    "#    - 선택된 g++로 -ccbin 강제\n",
    "#    - libstdc++ 헤더를 -isystem으로 우선 사용\n",
    "_orig_load = cpp_ext.load\n",
    "def _patched_load(name, sources, extra_cflags=None, extra_cuda_cflags=None, **kw):\n",
    "    ecf  = list(extra_cflags or [])\n",
    "    eccf = list(extra_cuda_cflags or [])\n",
    "    if CUDA_INC_CONDA.exists():\n",
    "        ecf.append(f\"-isystem{CUDA_INC_CONDA}\")\n",
    "\n",
    "    cleaned = []\n",
    "    i = 0\n",
    "    while i < len(eccf):\n",
    "        f = eccf[i]\n",
    "        if f == \"-ccbin\":\n",
    "            i += 2\n",
    "            continue\n",
    "        if str(f).startswith(\"-ccbin=\"):\n",
    "            i += 1\n",
    "            continue\n",
    "        cleaned.append(f)\n",
    "        i += 1\n",
    "    eccf = cleaned\n",
    "    eccf.append(f\"-ccbin={CONDAGXX}\")\n",
    "\n",
    "    if INC_BASE.exists(): ecf.append(f\"-isystem{INC_BASE}\")\n",
    "    if INC_TGT.exists():  ecf.append(f\"-isystem{INC_TGT}\")\n",
    "\n",
    "    kw.pop(\"verbose\", None)  # 중복 verbose 방지\n",
    "\n",
    "    print(f\"[patch] -ccbin={CONDAGXX}\")\n",
    "    if INC_BASE.exists(): print(f\"[patch] -isystem {INC_BASE}\")\n",
    "    if INC_TGT.exists():  print(f\"[patch] -isystem {INC_TGT}\")\n",
    "\n",
    "    return _orig_load(name, sources, extra_cflags=ecf, extra_cuda_cflags=eccf, verbose=True, **kw)\n",
    "\n",
    "cpp_ext.load = _patched_load\n",
    "print(\"OK: cpp_extension.load single-patched (toolchain pinned)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4e1a5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.387329"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nbfnet.models import NBFNet\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device =\", device)\n",
    "\n",
    "model = NBFNet(\n",
    "    input_dim=INPUT_DIM,\n",
    "    hidden_dims=HIDDEN_DIMS,\n",
    "    num_relation=NUM_RELATIONS,      # 2 (정/역)\n",
    "    message_func=MESSAGE_FUNCT,      # \"distmult\"\n",
    "    aggregate_func=AGGREGATE_FUNC,   # \"pna\"\n",
    "    short_cut=SHORTCUT,\n",
    "    layer_norm=LAYER_NORM,\n",
    "    activation=\"relu\",\n",
    "    concat_hidden=False,\n",
    "    num_mlp_layer=2,\n",
    "    dependent=True,\n",
    "    remove_one_hop=True,\n",
    "    num_beam=10,\n",
    "    path_topk=10,\n",
    ").to(device)\n",
    "\n",
    "# ★ 변경점: graph도 한 번만 옮겨놓고 이후에는 data 그대로 사용\n",
    "data = data.to(device)\n",
    "\n",
    "sum(p.numel() for p in model.parameters())/1e6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bf102e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def make_tail_neg_batch(pos_triples, num_nodes, num_neg, rel_id=0):\n",
    "    B = pos_triples.size(0)\n",
    "    h = pos_triples[:, 0]\n",
    "    t_pos = pos_triples[:, 2]\n",
    "    r = torch.full_like(h, fill_value=rel_id)\n",
    "\n",
    "    neg_tails = torch.randint(0, num_nodes, (B, num_neg), device=pos_triples.device)\n",
    "    mask = (neg_tails == t_pos.unsqueeze(1))\n",
    "    while mask.any():\n",
    "        neg_tails[mask] = torch.randint(0, num_nodes, (mask.sum(),), device=pos_triples.device)\n",
    "        mask = (neg_tails == t_pos.unsqueeze(1))\n",
    "\n",
    "    h_mat = h.unsqueeze(1).expand(-1, 1+num_neg)\n",
    "    t_mat = torch.cat([t_pos.unsqueeze(1), neg_tails], dim=1)\n",
    "    r_mat = torch.full_like(t_mat, fill_value=rel_id)\n",
    "\n",
    "    batch = torch.stack([h_mat, t_mat, r_mat], dim=-1).long()\n",
    "    return batch\n",
    "\n",
    "def bce_loss_from_scores(scores):\n",
    "    labels = torch.zeros_like(scores)\n",
    "    labels[:, 0] = 1.0\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_auc(model, data, triples, num_neg=64, rel_id=0, batch_size=4096):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    for i in range(0, len(triples), batch_size):\n",
    "        pos = torch.as_tensor(triples[i:i+batch_size], device=device)\n",
    "        batch = make_tail_neg_batch(pos, data.num_nodes, num_neg, rel_id=rel_id)\n",
    "        # ★ 변경점: data는 이미 device에 있음 → 매번 .to(device) 하지 않음\n",
    "        scores = model(data, batch).squeeze(-1)  # [B, 1+K]\n",
    "        pos_s = scores[:, 0:1]\n",
    "        neg_s = scores[:, 1:]\n",
    "        correct += (pos_s > neg_s).float().mean(dim=1).sum().item()\n",
    "        total += pos.size(0)\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0068af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU count visible to torch: 2\n",
      " [0] name=NVIDIA RTX A6000\n",
      " [1] name=NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "train_t = torch.as_tensor(train_triples, device=device)\n",
    "valid_t = torch.as_tensor(valid_triples, device=device)\n",
    "test_t  = torch.as_tensor(test_triples , device=device)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU count visible to torch:\", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\" [{i}] name={torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7e431d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load rspmm extension. This may take a while...\n",
      "[patch] -ccbin=/data2/project/bin_jip/miniconda3/envs/nbf37/bin/x86_64-conda-linux-gnu-g++\n",
      "[patch] -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/x86_64-conda-linux-gnu/include/c++/10.4.0\n",
      "[patch] -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/x86_64-conda-linux-gnu/include/c++/10.4.0/x86_64-conda-linux-gnu\n",
      "Using /data2/project/bin_jip/nbfnet_pyg/nbfnet_runs/torch_ext as PyTorch extensions root...\n",
      "Creating extension directory /data2/project/bin_jip/nbfnet_pyg/nbfnet_runs/torch_ext/rspmm...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /data2/project/bin_jip/nbfnet_pyg/nbfnet_runs/torch_ext/rspmm/build.ninja...\n",
      "Building extension module rspmm...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/lib/nvidia-cuda-toolkit/bin/nvcc --generate-dependencies-with-compile --dependency-output rspmm.cuda.o.d -ccbin /data2/project/bin_jip/miniconda3/envs/nbf37/bin/x86_64-conda-linux-gnu-gcc -DTORCH_EXTENSION_NAME=rspmm -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include/TH -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include/THC -isystem /usr/lib/nvidia-cuda-toolkit/include -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -ccbin=/data2/project/bin_jip/miniconda3/envs/nbf37/bin/x86_64-conda-linux-gnu-g++ -std=c++14 -c /data2/project/bin_jip/nbfnet_pyg/nbfnet/rspmm/source/rspmm.cu -o rspmm.cuda.o \n",
      "FAILED: [code=1] rspmm.cuda.o \n",
      "/usr/lib/nvidia-cuda-toolkit/bin/nvcc --generate-dependencies-with-compile --dependency-output rspmm.cuda.o.d -ccbin /data2/project/bin_jip/miniconda3/envs/nbf37/bin/x86_64-conda-linux-gnu-gcc -DTORCH_EXTENSION_NAME=rspmm -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include/TH -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include/THC -isystem /usr/lib/nvidia-cuda-toolkit/include -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -ccbin=/data2/project/bin_jip/miniconda3/envs/nbf37/bin/x86_64-conda-linux-gnu-g++ -std=c++14 -c /data2/project/bin_jip/nbfnet_pyg/nbfnet/rspmm/source/rspmm.cu -o rspmm.cuda.o \n",
      "nvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\n",
      "nvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\n",
      "cc1plus: fatal error: cuda_runtime.h: No such file or directory\n",
      "compilation terminated.\n",
      "[2/3] /data2/project/bin_jip/miniconda3/envs/nbf37/bin/x86_64-conda-linux-gnu-g++ -MMD -MF rspmm.o.d -DTORCH_EXTENSION_NAME=rspmm -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include/TH -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include/THC -isystem /usr/lib/nvidia-cuda-toolkit/include -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -Ofast -fopenmp -DAT_PARALLEL_OPENMP -DCUDA_OP -isystem/data2/project/bin_jip/miniconda3/envs/nbf37/targets/x86_64-linux/include -isystem/data2/project/bin_jip/miniconda3/envs/nbf37/x86_64-conda-linux-gnu/include/c++/10.4.0 -isystem/data2/project/bin_jip/miniconda3/envs/nbf37/x86_64-conda-linux-gnu/include/c++/10.4.0/x86_64-conda-linux-gnu -c /data2/project/bin_jip/nbfnet_pyg/nbfnet/rspmm/source/rspmm.cpp -o rspmm.o \n",
      "FAILED: [code=1] rspmm.o \n",
      "/data2/project/bin_jip/miniconda3/envs/nbf37/bin/x86_64-conda-linux-gnu-g++ -MMD -MF rspmm.o.d -DTORCH_EXTENSION_NAME=rspmm -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include/TH -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include/THC -isystem /usr/lib/nvidia-cuda-toolkit/include -isystem /data2/project/bin_jip/miniconda3/envs/nbf37/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -Ofast -fopenmp -DAT_PARALLEL_OPENMP -DCUDA_OP -isystem/data2/project/bin_jip/miniconda3/envs/nbf37/targets/x86_64-linux/include -isystem/data2/project/bin_jip/miniconda3/envs/nbf37/x86_64-conda-linux-gnu/include/c++/10.4.0 -isystem/data2/project/bin_jip/miniconda3/envs/nbf37/x86_64-conda-linux-gnu/include/c++/10.4.0/x86_64-conda-linux-gnu -c /data2/project/bin_jip/nbfnet_pyg/nbfnet/rspmm/source/rspmm.cpp -o rspmm.o \n",
      "In file included from /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include/torch/csrc/python_headers.h:10,\n",
      "                 from /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include/torch/csrc/Device.h:3,\n",
      "                 from /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/python.h:8,\n",
      "                 from /data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/include/torch/extension.h:6,\n",
      "                 from /data2/project/bin_jip/nbfnet_pyg/nbfnet/rspmm/source/rspmm.h:5,\n",
      "                 from /data2/project/bin_jip/nbfnet_pyg/nbfnet/rspmm/source/rspmm.cpp:6:\n",
      "/data2/project/bin_jip/miniconda3/envs/nbf37/include/python3.7m/Python.h:44:10: fatal error: crypt.h: No such file or directory\n",
      "   44 | #include <crypt.h>\n",
      "      |          ^~~~~~~~~\n",
      "compilation terminated.\n",
      "ninja: build stopped: subcommand failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error building extension 'rspmm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/utils/cpp_extension.py\u001b[0m in \u001b[0;36m_run_ninja_build\u001b[0;34m(build_directory, verbose, error_prefix)\u001b[0m\n\u001b[1;32m   1672\u001b[0m             \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m             env=env)\n\u001b[0m\u001b[1;32m   1674\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[0;32m--> 512\u001b[0;31m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['ninja', '-v']' returned non-zero exit status 1.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_396061/3502118386.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_tail_neg_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_NEG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, 1+K]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbce_loss_from_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/nbfnet_pyg/nbfnet/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, batch)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# message passing and updated node representations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbellmanford\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (num_nodes, batch_size, feature_dim）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"node_feature\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/nbfnet_pyg/nbfnet/models.py\u001b[0m in \u001b[0;36mbellmanford\u001b[0;34m(self, data, h_index, r_index, separate_grad)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0medge_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# Bellman-Ford iteration, we send the original boundary condition in addition to the updated node states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboundary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_cut\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlayer_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;31m# residual connection here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/nbfnet_pyg/nbfnet/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, query, boundary, edge_index, edge_type, size, edge_weight)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# correspond to Eq.6 on p5 in https://arxiv.org/pdf/2106.06935.pdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         output = self.propagate(input=input, relation=relation, boundary=boundary, edge_index=edge_index,\n\u001b[0;32m---> 66\u001b[0;31m                                 edge_type=edge_type, size=size, edge_weight=edge_weight)\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/nbfnet_pyg/nbfnet/layers.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_aggr_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_and_aggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmsg_aggr_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_and_aggregate_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_aggr_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/nbfnet_pyg/nbfnet/layers.py\u001b[0m in \u001b[0;36mmessage_and_aggregate\u001b[0;34m(self, edge_index, input, relation, boundary, edge_type, edge_weight, index, dim_size)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# speed up computation by several times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# reduce memory complexity from O(|E|d) to O(|V|d), so we can apply it to larger graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrspmm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeneralized_rspmm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/nbfnet_pyg/nbfnet/rspmm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrspmm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeneralized_rspmm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data2/project/bin_jip/nbfnet_pyg/nbfnet/rspmm/rspmm.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Load rspmm extension. This may take a while...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"source\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m \u001b[0mrspmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rspmm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rspmm.cpp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rspmm.cu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data2/project/bin_jip/nbfnet_pyg/nbfnet/rspmm/rspmm.py\u001b[0m in \u001b[0;36mload_extension\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0msources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcpp_extension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_cflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_cuda_cflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_396061/2955290340.py\u001b[0m in \u001b[0;36m_patched_load\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, **kw)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mINC_TGT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[patch] -isystem {INC_TGT}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_orig_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_cflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mecf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_cuda_cflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meccf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mcpp_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_patched_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/utils/cpp_extension.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0mis_python_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0mis_standalone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m         keep_intermediates=keep_intermediates)\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/utils/cpp_extension.py\u001b[0m in \u001b[0;36m_jit_compile\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1300\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m                         \u001b[0mwith_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_cuda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                         is_standalone=is_standalone)\n\u001b[0m\u001b[1;32m   1303\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m                 \u001b[0mbaton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/utils/cpp_extension.py\u001b[0m in \u001b[0;36m_write_ninja_file_and_build_library\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_standalone)\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0mbuild_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         error_prefix=f\"Error building extension '{name}'\")\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/utils/cpp_extension.py\u001b[0m in \u001b[0;36m_run_ninja_build\u001b[0;34m(build_directory, verbose, error_prefix)\u001b[0m\n\u001b[1;32m   1681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'output'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\": {error.output.decode()}\"\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error building extension 'rspmm'"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "best_val = -1\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    # ★ 변경점: randperm은 CPU에서 생성 (device 인자 제거)\n",
    "    idx = torch.randperm(train_t.size(0))\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for i in range(0, train_t.size(0), BATCH_SIZE):\n",
    "        pos = train_t[idx[i:i+BATCH_SIZE]]\n",
    "        batch = make_tail_neg_batch(pos, data.num_nodes, NUM_NEG, rel_id=0)\n",
    "        scores = model(data, batch)  # [B, 1+K]\n",
    "        loss = bce_loss_from_scores(scores)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * pos.size(0)\n",
    "    train_loss /= train_t.size(0)\n",
    "\n",
    "    val_metric = eval_auc(model, data, valid_triples, num_neg=64, rel_id=0, batch_size=8192)\n",
    "    dt = time.time() - t0\n",
    "    print(f\"[{epoch:02d}] loss={train_loss:.4f}  val@AUC≈{val_metric:.4f}  ({dt:.1f}s)\")\n",
    "    if val_metric > best_val:\n",
    "        best_val = val_metric\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "# best 로드 후 테스트 점검\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "test_metric = eval_auc(model, data, test_triples, num_neg=64, rel_id=0, batch_size=8192)\n",
    "print(f\"Test AUC≈{test_metric:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ab29330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAM50 present: 43 / 50  | missing: 3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GeneralizedRelationalConv' object has no attribute '__check_input__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_370051/2765725018.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mpam50_suggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msym\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpresent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank_neighbors_for_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTOPK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mpam50_suggestions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msym\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_to_symbol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_370051/2765725018.py\u001b[0m in \u001b[0;36mrank_neighbors_for_head\u001b[0;34m(model, data, h_idx, topk, rel_id, chunk)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_mat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [L]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mall_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mall_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/nbfnet_pyg/nbfnet/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, batch)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# message passing and updated node representations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbellmanford\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (num_nodes, batch_size, feature_dim）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"node_feature\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/nbfnet_pyg/nbfnet/models.py\u001b[0m in \u001b[0;36mbellmanford\u001b[0;34m(self, data, h_index, r_index, separate_grad)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0medge_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# Bellman-Ford iteration, we send the original boundary condition in addition to the updated node states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboundary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_cut\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlayer_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;31m# residual connection here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/nbfnet_pyg/nbfnet/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, query, boundary, edge_index, edge_type, size, edge_weight)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# correspond to Eq.6 on p5 in https://arxiv.org/pdf/2106.06935.pdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         output = self.propagate(input=input, relation=relation, boundary=boundary, edge_index=edge_index,\n\u001b[0;32m---> 66\u001b[0;31m                                 edge_type=edge_type, size=size, edge_weight=edge_weight)\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/project/bin_jip/nbfnet_pyg/nbfnet/layers.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__check_input__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         coll_dict = self.__collect__(self.__fused_user_args__, edge_index,\n\u001b[1;32m     82\u001b[0m                                      size, kwargs)\n",
      "\u001b[0;32m/data2/project/bin_jip/miniconda3/envs/nbf37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 948\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GeneralizedRelationalConv' object has no attribute '__check_input__'"
     ]
    }
   ],
   "source": [
    "PAM50 = [\n",
    "    \"BCL2\", \"BIRC5\", \"CCNB1\", \"CDC20\", \"CDH3\", \"CENPF\", \"CXXC5\", \"EGFR\", \"ERBB2\", \"ESR1\",\n",
    "    \"EXO1\", \"FGFR4\", \"FOXA1\", \"GRB7\", \"KIF2C\", \"KRT14\", \"KRT17\", \"KRT5\", \"MELK\", \"MIA\",\n",
    "    \"MK167\", \"MMP11\", \"MYBL2\", \"NAT1\", \"ORC6\", \"PGR\", \"PHGDH\", \"PTTG1\", \"RRM2\", \"SLC39A6\",\n",
    "    \"TMEM45B\", \"TYMS\", \"UBE2C\", \"UBE2T\", \"BAG1\", \"BLVRA\", \"CXXC5\", \"FOXA1\", \"MAPT\", \"MMP11\",\n",
    "    \"MMP7\", \"MMP9\", \"MYC\", \"NDC80\", \"NUF2\", \"SFRP1\", \"UBE2C\", \"CCNE1\", \"KRT8\", \"KRT18\"\n",
    "]\n",
    "\n",
    "def symbol_to_nodeidx(sym):\n",
    "    pid = sym_to_prot.get(sym)\n",
    "    if pid is None:\n",
    "        return None\n",
    "    return pid_to_idx.get(pid)\n",
    "\n",
    "pam50_nodes = {sym: symbol_to_nodeidx(sym) for sym in PAM50}\n",
    "present = {sym: idx for sym, idx in pam50_nodes.items() if idx is not None}\n",
    "missing = [sym for sym, idx in pam50_nodes.items() if idx is None]\n",
    "print(f\"PAM50 present: {len(present)} / 50  | missing: {len(missing)}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def rank_neighbors_for_head(model, data, h_idx, topk=TOPK, rel_id=0, chunk=8192):\n",
    "    model.eval()\n",
    "    all_scores, all_nodes = [], []\n",
    "    N = data.num_nodes\n",
    "    h = torch.full((1,), h_idx, dtype=torch.long, device=device)\n",
    "    r = torch.full((1,), rel_id, dtype=torch.long, device=device)\n",
    "    for start in range(0, N, chunk):\n",
    "        cand = torch.arange(start, min(start+chunk, N), device=device)\n",
    "        h_mat = h.unsqueeze(1).expand(1, cand.numel())\n",
    "        t_mat = cand.unsqueeze(0)\n",
    "        r_mat = r.unsqueeze(1).expand(1, cand.numel())\n",
    "\n",
    "        batch = torch.stack([h_mat, t_mat, r_mat], dim=-1).long()\n",
    "\n",
    "        scores = model(data, batch).squeeze(0)  # [L]\n",
    "        all_scores.append(scores.detach().cpu())\n",
    "        all_nodes.append(cand.detach().cpu())\n",
    "    scores = torch.cat(all_scores)\n",
    "    nodes  = torch.cat(all_nodes)\n",
    "\n",
    "    u = h_idx\n",
    "    known_v = set(edges[edges[:,0] == u][:,1].tolist())\n",
    "    known_v.add(u)\n",
    "    mask = torch.tensor([n not in known_v for n in nodes.tolist()])\n",
    "    scores = scores[mask]; nodes = nodes[mask]\n",
    "\n",
    "    topk_idx = torch.topk(scores, k=min(topk, scores.numel())).indices\n",
    "    top_nodes = nodes[topk_idx].tolist()\n",
    "    top_scores = scores[topk_idx].tolist()\n",
    "    return list(zip(top_nodes, top_scores))\n",
    "\n",
    "def idx_to_symbol(node_idx):\n",
    "    pid = idx_to_pid[node_idx]\n",
    "    return prot_to_sym.get(pid, pid)\n",
    "\n",
    "pam50_suggestions = {}\n",
    "for sym, idx in present.items():\n",
    "    pairs = rank_neighbors_for_head(model, data, idx, topk=TOPK, rel_id=0)\n",
    "    pam50_suggestions[sym] = [(idx_to_symbol(n), float(s)) for n, s in pairs]\n",
    "\n",
    "import pandas as pd\n",
    "rows = []\n",
    "for sym, lst in pam50_suggestions.items():\n",
    "    for rank, (gsym, score) in enumerate(lst, start=1):\n",
    "        rows.append({\"seed_gene\": sym, \"rank\": rank, \"candidate_gene\": gsym, \"score\": score})\n",
    "pam50_df = pd.DataFrame(rows).sort_values([\"seed_gene\",\"rank\"])\n",
    "display(pam50_df.head(20))\n",
    "pam50_df.to_csv(RUN_DIR / f\"{EXPT_NAME}_pam50_top{TOPK}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f69c12a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca48295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbf37",
   "language": "python",
   "name": "nbf37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
